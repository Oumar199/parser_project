{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet Numéro 4 : Parser\n",
    "-------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sommaire :\n",
    "1. [Processing](#Processing)\n",
    "2. [Grammar_rules](#grammar_rules)\n",
    "2. [NP_chunks](#np_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer par écrire une fonction qui permet de traiter d'une phrase/texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parser_package/utils/processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parser_package/utils/processing.py\n",
    "from parser_package import *\n",
    "\n",
    "def preprocess(sentence: str):\n",
    "    \"\"\"Convert `sentence` to a list of its words.\n",
    "    Pre-process sentence by converting all characters to lowercase\n",
    "    and removing any word that does not contain at least one alphabetic\n",
    "    character.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Le texte à traiter\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = [token.lower() for token in nltk.word_tokenize(sentence) if token.isalpha()]\n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons la fonction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser_package.utils.processing import preprocess\n",
    "\n",
    "tokens = preprocess(\"Je suis la\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'suis', 'la']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar_rules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les symboles terminaux ont déja été initialisés. Nous n'aurons donc qu'à créer les symboles non terminaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINALS = \"\"\"\n",
    "Adj -> \"country\" | \"dreadful\" | \"enigmatical\" | \"little\" | \"moist\" | \"red\"\n",
    "Adv -> \"down\" | \"here\" | \"never\"\n",
    "Conj -> \"and\" | \"until\"\n",
    "Det -> \"a\" | \"an\" | \"his\" | \"my\" | \"the\"\n",
    "N -> \"armchair\" | \"companion\" | \"day\" | \"door\" | \"hand\" | \"he\" | \"himself\"\n",
    "N -> \"holmes\" | \"home\" | \"i\" | \"mess\" | \"paint\" | \"palm\" | \"pipe\" | \"she\"\n",
    "N -> \"smile\" | \"thursday\" | \"walk\" | \"we\" | \"word\"\n",
    "P -> \"at\" | \"before\" | \"in\" | \"of\" | \"on\" | \"to\"\n",
    "V -> \"arrived\" | \"came\" | \"chuckled\" | \"had\" | \"lit\" | \"said\" | \"sat\"\n",
    "V -> \"smiled\" | \"tell\" | \"were\"\n",
    "\"\"\"\n",
    "\n",
    "NONTERMINALS = \"\"\"\n",
    "S -> NP PP | NP VP | S Conj S\n",
    "PP -> P NP\n",
    "NP -> N | Det N | Det Adj N | NP Adv | NP PP | NP Conj NP \n",
    "VP -> V | V V | VP Adv | VP NP | VP PP | VP Conj VP\n",
    "Adj -> Adj Adj\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(NONTERMINALS + TERMINALS)\n",
    "parser = nltk.ChartParser(grammar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargeons les textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for file_ in glob(\"sentences/*.txt\"):\n",
    "    \n",
    "    with open(file_, \"r\") as f:\n",
    "    \n",
    "        sentences.append(f.read().strip())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I had a little moist red paint in the palm of my hand.',\n",
       " 'I had a country walk on Thursday and came home in a dreadful mess.',\n",
       " 'Holmes sat down and lit his pipe.',\n",
       " 'My companion smiled an enigmatical smile.',\n",
       " 'Holmes sat in the red armchair and he chuckled.',\n",
       " 'Holmes chuckled to himself.',\n",
       " 'She never said a word until we were at the door here.',\n",
       " 'We arrived the day before Thursday.',\n",
       " 'Holmes lit a pipe.',\n",
       " 'Holmes sat.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NP_Chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser la recherche en profondeur pour trouver les sous arbres de symbole terminal $NP$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile parser_package/utils/np_chunks.py\n",
    "\n",
    "def np_chunk(tree):\n",
    "    \"\"\"\n",
    "    Return a list of all noun phrase chunks in the sentence tree.\n",
    "    A noun phrase chunk is defined as any subtree of the sentence\n",
    "    whose label is \"NP\" that does not itself contain any other\n",
    "    noun phrases as subtrees.\n",
    "    \"\"\"\n",
    "    \n",
    "    # let's take the subtrees of the tree\n",
    "    subtrees = list(tree.subtrees())\n",
    "    \n",
    "    # let's initialize \n",
    "    \n",
    "    # let's initialize the marked subtrees\n",
    "    marked = \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked = deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked.append(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked.pop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grammarrules-R9LSO1qf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350e372a59c6a2174c4996d01ba34ddc044542d9850a5b2a01946a38bca0736a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
